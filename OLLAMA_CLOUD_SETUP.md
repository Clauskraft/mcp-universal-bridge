# ü¶ô Ollama Cloud Hosting - Setup Guide

**Problem:** Railway kan ikke k√∏re Ollama direkte (mangler GPU support og storage for modeller)

**L√∏sning:** Host Ollama p√• en dedikeret server eller cloud service

---

## üöÄ Option 1: Replicate (Anbefalet - Nemmest)

**Fordele:**
- ‚úÖ Pay-per-use pricing
- ‚úÖ Ingen server management
- ‚úÖ GPU acceleration inkluderet
- ‚úÖ API compatible med Ollama

**Setup:**
1. G√• til https://replicate.com
2. Opret konto og f√• API key
3. Brug Replicate API til at k√∏re Llama modeller
4. Opdater din kode til at bruge Replicate endpoint

**Pricing:**
- ~$0.0005 per sekund GPU tid
- Meget billigere end Claude/ChatGPT for simple tasks

---

## üåê Option 2: Modal Labs (Serverless GPU)

**Fordele:**
- ‚úÖ Serverless GPU functions
- ‚úÖ Kan deploye din egen Ollama instance
- ‚úÖ Auto-scaling
- ‚úÖ Gratis tier available

**Setup:**
```python
# modal_ollama.py
from modal import Image, Stub, web_endpoint

stub = Stub("ollama-server")

ollama_image = Image.debian_slim().run_commands(
    "curl -fsSL https://ollama.com/install.sh | sh",
    "ollama pull llama3.3"
)

@stub.function(image=ollama_image, gpu="T4")
@web_endpoint(method="POST")
def chat(request):
    # Proxy requests to local Ollama
    import subprocess
    result = subprocess.run(
        ["ollama", "run", request["model"]],
        input=request["prompt"],
        capture_output=True,
        text=True
    )
    return {"response": result.stdout}
```

Deploy:
```bash
modal deploy modal_ollama.py
# F√•r URL: https://username--ollama-server-chat.modal.run
```

---

## üñ•Ô∏è Option 3: Separat VPS Server (Mest kontrol)

**Providers:**
- **Hetzner Cloud** - ‚Ç¨4.50/m√•ned (2 vCPU, 4GB RAM)
- **DigitalOcean** - $6/m√•ned (1GB RAM, basic tier)
- **Vultr** - $6/m√•ned (1GB RAM, SSD)

**Setup p√• VPS:**

```bash
# 1. SSH ind p√• din server
ssh root@your-server-ip

# 2. Installer Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 3. Pull modeller
ollama pull llama3.3
ollama pull mistral

# 4. Start Ollama med external access
OLLAMA_HOST=0.0.0.0:11434 ollama serve &

# 5. Tillad port 11434 i firewall
ufw allow 11434

# 6. Test fra din computer
curl http://your-server-ip:11434/api/tags
```

**Railway Environment Variable:**
```bash
OLLAMA_BASE_URL=http://your-server-ip:11434
```

---

## üê≥ Option 4: Ollama i Separate Railway Service

Railway underst√∏tter Docker containers med persistent storage:

**Create nixpacks.toml:**
```toml
[phases.setup]
nixPkgs = ["curl", "ca-certificates"]

[phases.install]
cmds = ["curl -fsSL https://ollama.com/install.sh | sh"]

[phases.build]
cmds = ["ollama pull llama3.3:latest"]

[start]
cmd = "OLLAMA_HOST=0.0.0.0:11434 ollama serve"
```

**Railway Setup:**
1. Opret ny Railway service fra repo
2. Link service til din MCP Bridge
3. Brug internal Railway URL
4. Set environment variable: `OLLAMA_BASE_URL=https://ollama-service.railway.internal:11434`

‚ö†Ô∏è **Bem√¶rk:** Railway's free tier har storage limits - modeller kan blive slettet ved restart

---

## üéØ Anbefaling for Produktion

### For Udvikling & Testing:
- ‚úÖ **K√∏r Ollama lokalt** p√• din PC (gratis, ingen setup)

### For Lav Trafik (<100 requests/dag):
- ‚úÖ **Replicate** - Pay per use, ingen fixed costs

### For Mellem Trafik (100-1000 requests/dag):
- ‚úÖ **Hetzner VPS** - ‚Ç¨4.50/m√•ned, ubegr√¶nset brug

### For H√∏j Trafik (>1000 requests/dag):
- ‚úÖ **Modal Labs** - Auto-scaling serverless
- ‚úÖ Eller brug kun cloud providers (Claude/ChatGPT/Gemini)

---

## üîß Implementation: Replicate Integration

Det nemmeste er at bruge Replicate's API:

**Opdater `src/providers/ollama.ts`:**

```typescript
async chat(request: ChatRequest, session: Session): Promise<ChatResponse> {
  // Check if using cloud Ollama provider
  const useReplicate = this.baseURL.includes('replicate.com');

  if (useReplicate) {
    // Use Replicate API
    const response = await fetch('https://api.replicate.com/v1/predictions', {
      method: 'POST',
      headers: {
        'Authorization': `Token ${process.env.REPLICATE_API_TOKEN}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        version: 'meta/llama-3.3-70b-instruct',
        input: {
          prompt: request.message
        }
      })
    });
    // Process response...
  } else {
    // Use local Ollama
    // ... existing code
  }
}
```

**Environment Variables p√• Railway:**
```bash
REPLICATE_API_TOKEN=r8_your_token_here
OLLAMA_BASE_URL=https://api.replicate.com
```

---

## ‚úÖ Hurtig Start Guide

**For at f√• Ollama til at virke p√• Railway NU:**

1. **V√¶lg l√∏sning:**
   - **Nemmest:** Replicate (betal per brug)
   - **Billigst long-term:** Hetzner VPS ‚Ç¨4.50/m√•ned
   - **Dev/test:** K√∏r lokalt p√• din PC

2. **Configure Railway:**
   ```bash
   # Tilf√∏j environment variable p√• Railway dashboard:
   OLLAMA_BASE_URL=https://your-chosen-service-url
   ```

3. **Test:**
   ```bash
   curl https://web-production-d9b2.up.railway.app/providers/ollama/models
   ```

---

**Hvad vil du g√∏re?**
- A) Setup Replicate integration (nemmest, pay-per-use)
- B) Deploy Ollama til Hetzner VPS (billigst for produktion)
- C) Brug separate Railway service (medium kompleksitet)
- D) Hold Ollama kun til lokal udvikling
